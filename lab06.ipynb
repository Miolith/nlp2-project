{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "577261f15579496faf4e770f424ebf7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dcd8bdf4768f44afbc92261d30ca20e6",
              "IPY_MODEL_81e81e967f5447e4965d73bdf9dfeba2",
              "IPY_MODEL_1fc764487be74e919a0b5dcefbe87ae6"
            ],
            "layout": "IPY_MODEL_bb9ca90b9c254cb798906a6c507ce119"
          }
        },
        "dcd8bdf4768f44afbc92261d30ca20e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dc4b45e4bb243468196a74da0902fa4",
            "placeholder": "​",
            "style": "IPY_MODEL_cd6492570ed24cc79573c06aca51af59",
            "value": "100%"
          }
        },
        "81e81e967f5447e4965d73bdf9dfeba2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db3057256e7d4068ab8215f28f0da785",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d6508f34a4745c39549b33acc8d679e",
            "value": 3
          }
        },
        "1fc764487be74e919a0b5dcefbe87ae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f12a9ccd404b48f192f24f8254fe49dc",
            "placeholder": "​",
            "style": "IPY_MODEL_a5a24b9dfeb04b2fb550001b973ead61",
            "value": " 3/3 [00:00&lt;00:00, 55.71it/s]"
          }
        },
        "bb9ca90b9c254cb798906a6c507ce119": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dc4b45e4bb243468196a74da0902fa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd6492570ed24cc79573c06aca51af59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db3057256e7d4068ab8215f28f0da785": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d6508f34a4745c39549b33acc8d679e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f12a9ccd404b48f192f24f8254fe49dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5a24b9dfeb04b2fb550001b973ead61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## NLP LAB06\n",
        "- Nelson Vicel-Farrah\n",
        "- Karen Kaspar\n",
        "- Romain Brand"
      ],
      "metadata": {
        "id": "qsiWrXSyxofq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Fine-tune the model on the training data"
      ],
      "metadata": {
        "id": "GL4xLbQ0QEdP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QonrQhfFphUM",
        "outputId": "fd01a14f-2b99-451a-d817-ce9084e705b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.7/dist-packages (0.3.0)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.1.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.13.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.10.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.13.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.1.97)\n",
            "Requirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.19.6)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "installing the Transformers packages needed for this lab\n",
        "\"\"\"\n",
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "we use the recommened distilbert pre-trained model\n",
        "\"\"\"\n",
        "import torch\n",
        "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ],
      "metadata": {
        "id": "hTEV8BOqq1sV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "we load the imdb dataset which includes a test, train and unsupervised \n",
        "datasets of text and labels indicating if the text has a positive or negative connotation\n",
        "\"\"\"\n",
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"imdb\")\n",
        "raw_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "577261f15579496faf4e770f424ebf7f",
            "dcd8bdf4768f44afbc92261d30ca20e6",
            "81e81e967f5447e4965d73bdf9dfeba2",
            "1fc764487be74e919a0b5dcefbe87ae6",
            "bb9ca90b9c254cb798906a6c507ce119",
            "0dc4b45e4bb243468196a74da0902fa4",
            "cd6492570ed24cc79573c06aca51af59",
            "db3057256e7d4068ab8215f28f0da785",
            "4d6508f34a4745c39549b33acc8d679e",
            "f12a9ccd404b48f192f24f8254fe49dc",
            "a5a24b9dfeb04b2fb550001b973ead61"
          ]
        },
        "id": "p5KRID9BrHLC",
        "outputId": "e3093f0d-3d86-4f05-ae86-ab0f2e079e81"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "577261f15579496faf4e770f424ebf7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    unsupervised: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "function that is mapped on all the elements of the dataset in order to tokenize them\n",
        "    :param example: \n",
        "        dictionary containg the items of the dataset\n",
        "    :return: \n",
        "        returns a tokenized version of the dataset \n",
        "\"\"\"\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True)"
      ],
      "metadata": {
        "id": "tSh0qooIrLhx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL3tGXgyrPfZ",
        "outputId": "4420b9e6-d2e0-4eb8-bb91-1e9eb134da61"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-dfe85cfbad012780.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-b4dda65c3852ebe8.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-c9312d2b840c0340.arrow\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    unsupervised: Dataset({\n",
              "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "3gGEb9M4rWqu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")"
      ],
      "metadata": {
        "id": "QAOvSpyhr9wN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHJrSbwjsEhY",
        "outputId": "c0402cd5-187d-41e7-a931-b27e7ba2452c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "\"\"\"\n",
        "we use the accuracy in order to evaluate our model\n",
        "\"\"\"\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4NtVCAxAsGId",
        "outputId": "91f71a23-b7bd-4413-ad88-1a8e1e8b0c10"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 25000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 9375\n",
            "  Number of trainable parameters = 66955010\n",
            "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9375' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9375/9375 1:20:24, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.288000</td>\n",
              "      <td>0.236560</td>\n",
              "      <td>0.911080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.167800</td>\n",
              "      <td>0.304135</td>\n",
              "      <td>0.930120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.052300</td>\n",
              "      <td>0.373167</td>\n",
              "      <td>0.930920</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to test-trainer/checkpoint-500\n",
            "Configuration saved in test-trainer/checkpoint-500/config.json\n",
            "Model weights saved in test-trainer/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in test-trainer/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in test-trainer/checkpoint-500/special_tokens_map.json\n",
            "Saving model checkpoint to test-trainer/checkpoint-1000\n",
            "Configuration saved in test-trainer/checkpoint-1000/config.json\n",
            "Model weights saved in test-trainer/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in test-trainer/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in test-trainer/checkpoint-1000/special_tokens_map.json\n",
            "Saving model checkpoint to test-trainer/checkpoint-1500\n",
            "Configuration saved in test-trainer/checkpoint-1500/config.json\n",
            "Model weights saved in test-trainer/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in test-trainer/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in test-trainer/checkpoint-1500/special_tokens_map.json\n",
            "Saving model checkpoint to test-trainer/checkpoint-2000\n",
            "Configuration saved in test-trainer/checkpoint-2000/config.json\n",
            "Model weights saved in test-trainer/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in test-trainer/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in test-trainer/checkpoint-2000/special_tokens_map.json\n",
            "Saving model checkpoint to test-trainer/checkpoint-2500\n",
            "Configuration saved in test-trainer/checkpoint-2500/config.json\n",
            "Model weights saved in test-trainer/checkpoint-2500/pytorch_model.bin\n",
            "tokenizer config file saved in test-trainer/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in test-trainer/checkpoint-2500/special_tokens_map.json\n",
            "Saving model checkpoint to test-trainer/checkpoint-3000\n",
            "Configuration saved in test-trainer/checkpoint-3000/config.json\n",
            "Model weights saved in test-trainer/checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in test-trainer/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in test-trainer/checkpoint-3000/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 25000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-trainer/checkpoint-3500\n",
            "Configuration saved in test-trainer/checkpoint-3500/config.json\n",
            "Model weights saved in test-trainer/checkpoint-3500/pytorch_model.bin\n",
            "tokenizer config file saved in test-trainer/checkpoint-3500/tokenizer_config.json\n",
            "Special tokens file saved in test-trainer/checkpoint-3500/special_tokens_map.json\n",
            "Saving model checkpoint to test-trainer/checkpoint-4000\n",
            "Configuration saved in test-trainer/checkpoint-4000/config.json\n",
            "Model weights saved in test-trainer/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in test-trainer/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in test-trainer/checkpoint-4000/special_tokens_map.json\n",
            "Saving model checkpoint to test-trainer/checkpoint-4500\n",
            "Configuration saved in test-trainer/checkpoint-4500/config.json\n",
            "Model weights saved in test-trainer/checkpoint-4500/pytorch_model.bin\n",
            "tokenizer config file saved in test-trainer/checkpoint-4500/tokenizer_config.json\n",
            "Special tokens file saved in test-trainer/checkpoint-4500/special_tokens_map.json\n",
            "Saving model checkpoint to test-trainer/checkpoint-5000\n",
            "Configuration saved in test-trainer/checkpoint-5000/config.json\n",
            "Model weights saved in test-trainer/checkpoint-5000/pytorch_model.bin\n",
            "tokenizer config file saved in test-trainer/checkpoint-5000/tokenizer_config.json\n",
            "Special tokens file saved in test-trainer/checkpoint-5000/special_tokens_map.json\n",
            "Saving model checkpoint to test-trainer/checkpoint-5500\n",
            "Configuration saved in test-trainer/checkpoint-5500/config.json\n",
            "Model weights saved in test-trainer/checkpoint-5500/pytorch_model.bin\n",
            "tokenizer config file saved in test-trainer/checkpoint-5500/tokenizer_config.json\n",
            "Special tokens file saved in test-trainer/checkpoint-5500/special_tokens_map.json\n",
            "Saving model checkpoint to test-trainer/checkpoint-6000\n",
            "Configuration saved in test-trainer/checkpoint-6000/config.json\n",
            "Model weights saved in test-trainer/checkpoint-6000/pytorch_model.bin\n",
            "tokenizer config file saved in test-trainer/checkpoint-6000/tokenizer_config.json\n",
            "Special tokens file saved in test-trainer/checkpoint-6000/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 25000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-trainer/checkpoint-6500\n",
            "Configuration saved in test-trainer/checkpoint-6500/config.json\n",
            "Model weights saved in test-trainer/checkpoint-6500/pytorch_model.bin\n",
            "tokenizer config file saved in test-trainer/checkpoint-6500/tokenizer_config.json\n",
            "Special tokens file saved in test-trainer/checkpoint-6500/special_tokens_map.json\n",
            "Saving model checkpoint to test-trainer/checkpoint-7000\n",
            "Configuration saved in test-trainer/checkpoint-7000/config.json\n",
            "Model weights saved in test-trainer/checkpoint-7000/pytorch_model.bin\n",
            "tokenizer config file saved in test-trainer/checkpoint-7000/tokenizer_config.json\n",
            "Special tokens file saved in test-trainer/checkpoint-7000/special_tokens_map.json\n",
            "Saving model checkpoint to test-trainer/checkpoint-7500\n",
            "Configuration saved in test-trainer/checkpoint-7500/config.json\n",
            "Model weights saved in test-trainer/checkpoint-7500/pytorch_model.bin\n",
            "tokenizer config file saved in test-trainer/checkpoint-7500/tokenizer_config.json\n",
            "Special tokens file saved in test-trainer/checkpoint-7500/special_tokens_map.json\n",
            "Saving model checkpoint to test-trainer/checkpoint-8000\n",
            "Configuration saved in test-trainer/checkpoint-8000/config.json\n",
            "Model weights saved in test-trainer/checkpoint-8000/pytorch_model.bin\n",
            "tokenizer config file saved in test-trainer/checkpoint-8000/tokenizer_config.json\n",
            "Special tokens file saved in test-trainer/checkpoint-8000/special_tokens_map.json\n",
            "Saving model checkpoint to test-trainer/checkpoint-8500\n",
            "Configuration saved in test-trainer/checkpoint-8500/config.json\n",
            "Model weights saved in test-trainer/checkpoint-8500/pytorch_model.bin\n",
            "tokenizer config file saved in test-trainer/checkpoint-8500/tokenizer_config.json\n",
            "Special tokens file saved in test-trainer/checkpoint-8500/special_tokens_map.json\n",
            "Saving model checkpoint to test-trainer/checkpoint-9000\n",
            "Configuration saved in test-trainer/checkpoint-9000/config.json\n",
            "Model weights saved in test-trainer/checkpoint-9000/pytorch_model.bin\n",
            "tokenizer config file saved in test-trainer/checkpoint-9000/tokenizer_config.json\n",
            "Special tokens file saved in test-trainer/checkpoint-9000/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 25000\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=9375, training_loss=0.17382249572753905, metrics={'train_runtime': 4825.7096, 'train_samples_per_second': 15.542, 'train_steps_per_second': 1.943, 'total_flos': 9363658844900448.0, 'train_loss': 0.17382249572753905, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Evaluate the model in term of accuracy on the test data."
      ],
      "metadata": {
        "id": "JCKK0GKDQSJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "we calculae our model's accuracy\n",
        "\"\"\"\n",
        "\n",
        "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
        "print(predictions.predictions.shape, predictions.label_ids.shape)\n",
        "\n",
        "preds = np.argmax(predictions.predictions, axis=-1)\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "metric.compute(predictions=preds, references=predictions.label_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "S8UfBkjECKMP",
        "outputId": "51fc3aad-3f2a-47c0-bdaa-98d593358479"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 25000\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 2) (25000,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.93092}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. For at least 2 samples which have been wrongly classified in the test set, try explaining why the model could have been wrong."
      ],
      "metadata": {
        "id": "OrQJR4DJQbB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_labels = preds\n",
        "test_labels = np.array(tokenized_datasets['test']['label'])\n",
        "test_text = np.array(tokenized_datasets['test']['text'])\n",
        "\n",
        "number_wrong_examples = 0\n",
        "examples = []\n",
        "\n",
        "for index, value in enumerate(test_text):\n",
        "  if (number_wrong_examples == 2):\n",
        "    break\n",
        "  if (test_labels[index]!=prediction_labels[index]):\n",
        "    number_wrong_examples+=1\n",
        "    examples.append((value, test_labels[index], prediction_labels[index]))\n",
        "\n",
        "\n",
        "for text, label, prediction in examples:\n",
        "  print('text:', text)\n",
        "  print('label:', label)\n",
        "  print('prediction:', prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoVlqEDKDawB",
        "outputId": "273b7755-27c5-4ae4-907f-ca88effb5887"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: First off let me say, If you haven't enjoyed a Van Damme movie since bloodsport, you probably will not like this movie. Most of these movies may not have the best plots or best actors but I enjoy these kinds of movies for what they are. This movie is much better than any of the movies the other action guys (Segal and Dolph) have thought about putting out the past few years. Van Damme is good in the movie, the movie is only worth watching to Van Damme fans. It is not as good as Wake of Death (which i highly recommend to anyone of likes Van Damme) or In hell but, in my opinion it's worth watching. It has the same type of feel to it as Nowhere to Run. Good fun stuff!\n",
            "label: 0\n",
            "prediction: 1\n",
            "text: Ben, (Rupert Grint), is a deeply unhappy adolescent, the son of his unhappily married parents. His father, (Nicholas Farrell), is a vicar and his mother, (Laura Linney), is ... well, let's just say she's a somewhat hypocritical soldier in Jesus' army. It's only when he takes a summer job as an assistant to a foul-mouthed, eccentric, once-famous and now-forgotten actress Evie Walton, (Julie Walters), that he finally finds himself in true 'Harold and Maude' fashion. Of course, Evie is deeply unhappy herself and it's only when these two sad sacks find each other that they can put their mutual misery aside and hit the road to happiness.<br /><br />Of course it's corny and sentimental and very predictable but it has a hard side to it, too and Walters, who could sleep-walk her way through this sort of thing if she wanted, is excellent. It's when she puts the craziness to one side and finds the pathos in the character, (like hitting the bottle and throwing up in the sink), that she's at her best. The problem is she's the only interesting character in the film (and it's not because of the script which doesn't do anybody any favours). Grint, on the other hand, isn't just unhappy; he's a bit of a bore as well while Linney's starched bitch is completely one-dimensional. (Still, she's got the English accent off pat). The best that can be said for it is that it's mildly enjoyable - with the emphasis on the mildly.\n",
            "label: 0\n",
            "prediction: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both wrongly classified examples, are very long and complex. We also notice that the text is ambiguous and  it includes misleading sentences such as 'you will not like this movie' followed by 'it's worth watching' and 'good fun stuff' making it even impossible for humans to correctly label it. \n",
        "The model uses a bidirectional encoder, therefore the presence of ambiguous, alternating positive and negative sentences can explain the wrongly classified examples."
      ],
      "metadata": {
        "id": "MdnK-CXyQgHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. What are the advantages and inconvenient of using this model in production compared to the naive Bayes we implemented in the first part of the course?"
      ],
      "metadata": {
        "id": "Kn8O0BHUSlb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main advantage of using the naive Bayes we implemented in the first part of the course is that it's a lot faster to train. It is simple and easy to implement and doesn't require as much training data. It is also fast and can be used to make real-time predictions. However, it is not as effective and precise as using a model in production, which can be fine-tuned for the exact usage we need. And while the model in production required more time, data and ressources, it displays a better performance."
      ],
      "metadata": {
        "id": "su5952YQyv6d"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NfKpWppbXaqJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}