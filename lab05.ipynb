{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rn4qRoEJTpb"
      },
      "source": [
        "# Introduction to Natural Language Processing 2 Lab02"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZNEmJcmdQ-Zf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%matplotlib inline\n",
        "!pip install torch==1.13.0 spacy sacrebleu torchdata torchtext -U\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX0a2PniQ-Zj"
      },
      "source": [
        "\n",
        "# Language Translation with nn.Transformer and torchtext\n",
        "\n",
        "This tutorial shows:\n",
        "    - How to train a translation model from scratch using Transformer. \n",
        "    - Use tochtext library to access  [Multi30k](http://www.statmt.org/wmt16/multimodal-task.html#task1)_ dataset to train a German to English translation model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQi-r4HoQ-Zl"
      },
      "source": [
        "## Data Sourcing and Processing\n",
        "\n",
        "[torchtext library](https://pytorch.org/text/stable/)_ has utilities for creating datasets that can be easily\n",
        "iterated through for the purposes of creating a language translation\n",
        "model. In this example, we show how to use torchtext's inbuilt datasets, \n",
        "tokenize a raw text sentence, build vocabulary, and numericalize tokens into tensor. We will use\n",
        "[Multi30k dataset from torchtext library](https://pytorch.org/text/stable/datasets.html#multi30k)_\n",
        "that yields a pair of source-target raw sentences. \n",
        "\n",
        "To access torchtext datasets, please install torchdata following instructions at https://github.com/pytorch/data. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iFUaMZ5gQ-Zm"
      },
      "outputs": [],
      "source": [
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from typing import Iterable, List\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}\n",
        "\n",
        "\n",
        "# Create source and target language tokenizer.\n",
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "\n",
        "# helper function to yield list of tokens\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        " \n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    # Training data Iterator \n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    # Create torchtext's Vocab object \n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                    min_freq=1,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)\n",
        "\n",
        "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
        "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocab_transform[ln].set_default_index(UNK_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzi7AzX2Q-Zn"
      },
      "source": [
        "## Seq2Seq Network using Transformer\n",
        "\n",
        "Transformer is a Seq2Seq model introduced in [“Attention is all you\n",
        "need”](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)_\n",
        "paper for solving machine translation tasks. \n",
        "Below, we will create a Seq2Seq network that uses Transformer. The network\n",
        "consists of three parts. First part is the embedding layer. This layer converts tensor of input indices\n",
        "into corresponding tensor of input embeddings. These embedding are further augmented with positional\n",
        "encodings to provide position information of input tokens to the model. The second part is the \n",
        "actual [Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)_ model. \n",
        "Finally, the output of Transformer model is passed through linear layer\n",
        "that give un-normalized probabilities for each token in the target language. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9VkH7rkKQ-Zo"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network \n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, \n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFzVhAIkQ-Zp"
      },
      "source": [
        "During training, we need a subsequent word mask that will prevent model to look into\n",
        "the future words when making predictions. We will also need masks to hide\n",
        "source and target padding tokens. Below, let's define a function that will take care of both. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3kRzxHM2Q-Zq"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDMhqOgAQ-Zq"
      },
      "source": [
        "Let's now define the parameters of our model and instantiate the same. Below, we also \n",
        "define our loss function which is the cross-entropy loss and the optmizer used for training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kNck5W1gQ-Zr"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, \n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waRYe4Z1Q-Zr"
      },
      "source": [
        "## Collation\n",
        "\n",
        "As seen in the ``Data Sourcing and Processing`` section, our data iterator yields a pair of raw strings. \n",
        "We need to convert these string pairs into the batched tensors that can be processed by our ``Seq2Seq`` network \n",
        "defined previously. Below we define our collate function that convert batch of raw strings into batch tensors that\n",
        "can be fed directly into our model.   \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vmLX_6ZkQ-Zs"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), \n",
        "                      torch.tensor(token_ids), \n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# src and tgt language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tesors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI6g5-RAQ-Zs"
      },
      "source": [
        "Let's define training and evaluation loop that will be called for each \n",
        "epoch.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-WpLtd1KQ-Zt"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "    \n",
        "    for src, tgt in train_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(train_dataloader))\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in val_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "        \n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(val_dataloader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UmravlmQ-Zt"
      },
      "source": [
        "Now we have all the ingredients to train our model. Let's do it!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwMVKhBEQ-Zu",
        "outputId": "92cc944f-106b-459e-9510-4298f5a5b3b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 5.344, Val loss: 4.114, Epoch time = 45.280s\n",
            "Epoch: 2, Train loss: 3.761, Val loss: 3.320, Epoch time = 42.511s\n",
            "Epoch: 3, Train loss: 3.161, Val loss: 2.894, Epoch time = 43.232s\n",
            "Epoch: 4, Train loss: 2.768, Val loss: 2.638, Epoch time = 44.267s\n",
            "Epoch: 5, Train loss: 2.480, Val loss: 2.441, Epoch time = 42.927s\n",
            "Epoch: 6, Train loss: 2.250, Val loss: 2.315, Epoch time = 42.951s\n",
            "Epoch: 7, Train loss: 2.060, Val loss: 2.201, Epoch time = 44.267s\n",
            "Epoch: 8, Train loss: 1.897, Val loss: 2.113, Epoch time = 43.378s\n",
            "Epoch: 9, Train loss: 1.754, Val loss: 2.058, Epoch time = 43.213s\n",
            "Epoch: 10, Train loss: 1.631, Val loss: 2.002, Epoch time = 42.655s\n",
            "Epoch: 11, Train loss: 1.524, Val loss: 1.975, Epoch time = 43.084s\n",
            "Epoch: 12, Train loss: 1.420, Val loss: 1.945, Epoch time = 42.723s\n",
            "Epoch: 13, Train loss: 1.333, Val loss: 1.967, Epoch time = 43.049s\n",
            "Epoch: 14, Train loss: 1.251, Val loss: 1.942, Epoch time = 43.018s\n",
            "Epoch: 15, Train loss: 1.173, Val loss: 1.928, Epoch time = 43.077s\n",
            "Epoch: 16, Train loss: 1.103, Val loss: 1.914, Epoch time = 43.054s\n",
            "Epoch: 17, Train loss: 1.039, Val loss: 1.903, Epoch time = 44.294s\n",
            "Epoch: 18, Train loss: 0.979, Val loss: 1.907, Epoch time = 42.827s\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 18\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPa4vj1DJo6t"
      },
      "source": [
        "## (5 points) Decoding functions\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Greedy decoding"
      ],
      "metadata": {
        "id": "cF0yCm2C3KlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to generate output sequence using greedy algorithm \n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    \"\"\"\n",
        "    Greedy decoding algorithm\n",
        "\n",
        "    Args:\n",
        "        model: model\n",
        "        src: source sentence\n",
        "        src_mask: source mask\n",
        "        max_len: maximum length of output sentence\n",
        "        start_symbol: start symbol\n",
        "\n",
        "    Returns:\n",
        "        output sentence\n",
        "    \"\"\"\n",
        "\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys"
      ],
      "metadata": {
        "id": "EI9sZy573Poz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABoCHBaqKDN_"
      },
      "source": [
        "### Top K sampling with temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_DQeilOZKB9I"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def top_k_sampling_temperature(model, src, src_mask, max_len, start_symbol, k=5, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Top-k sampling with temperature\n",
        "\n",
        "    Args:\n",
        "        model: model\n",
        "        src: source sentence\n",
        "        src_mask: source mask\n",
        "        max_len: maximum length of output sentence\n",
        "        start_symbol: start symbol\n",
        "        k: top-k sampling parameter\n",
        "        temperature: float value between 0 and 1\n",
        "\n",
        "    Returns:\n",
        "        output sentence\n",
        "    \"\"\"\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        prob = F.softmax(prob / temperature, dim=1)\n",
        "        top_k_prob, top_k_index = torch.topk(prob, k=k, dim=1)\n",
        "        random_index = torch.multinomial(top_k_prob, 1)\n",
        "        \n",
        "        next_word = top_k_index[0][random_index].item()\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZjdwlweJ-C7"
      },
      "source": [
        "### Top k sampling\n",
        "\n",
        "On réutilise le *top k sampling* avec temperature en définissant la température à 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "J5qW2ml-lBpY"
      },
      "outputs": [],
      "source": [
        "def top_k_sampling(model, src, src_mask, max_len, start_symbol, k=5):\n",
        "    \"\"\"\n",
        "    Top-k sampling without temperature\n",
        "\n",
        "    Args:\n",
        "        model: model\n",
        "        src: source sentence\n",
        "        src_mask: source mask\n",
        "        max_len: maximum length of output sentence\n",
        "        start_symbol: start symbol\n",
        "        k: top-k sampling parameter\n",
        "\n",
        "    Returns:\n",
        "        output sentence\n",
        "    \"\"\"\n",
        "    return top_k_sampling_temperature(model, src, src_mask, max_len, start_symbol, k=k, temperature=1.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JZKaX8XKTYH"
      },
      "source": [
        "### Beam Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hqBvrqM8KWii"
      },
      "outputs": [],
      "source": [
        "def beam_search_sampling(model, src, src_mask, max_len, start_symbol, k=5):\n",
        "    \"\"\"\n",
        "    Beam search sampling\n",
        "\n",
        "    Args:\n",
        "        model: model\n",
        "        src: source sentence\n",
        "        src_mask: source mask\n",
        "        max_len: maximum length of output sentence\n",
        "        start_symbol: start symbol\n",
        "        k: beam size\n",
        "\n",
        "    Returns:\n",
        "        output sentence\n",
        "    \"\"\"\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "\n",
        "    # initialize the first hypothesis\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    memory = memory.to(DEVICE)\n",
        "    tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                .type(torch.bool)).to(DEVICE)\n",
        "    out = model.decode(ys, memory, tgt_mask)\n",
        "    out = out.transpose(0, 1)\n",
        "    prob = model.generator(out[:, -1])\n",
        "    prob = F.softmax(prob, dim=1)\n",
        "    top_k_prob, top_k_index = torch.topk(prob, k=k, dim=1)\n",
        " \n",
        "    # initialize the rest of the hypothesis\n",
        "    hypothesis = []\n",
        "    for i in range(k):\n",
        "        next_word = top_k_index[0][i].item()\n",
        "        hypothesis.append({\n",
        "            \"sequence\": torch.cat([ys,\n",
        "                    torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0),\n",
        "            \"score\": top_k_prob[0][i].item(),\n",
        "            \"memory\": memory\n",
        "        })\n",
        "\n",
        "    for i in range(max_len-1):\n",
        "        new_hypothesis = []\n",
        "        for h in hypothesis:\n",
        "            ys = h[\"sequence\"]\n",
        "            memory = h[\"memory\"]\n",
        "            tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                        .type(torch.bool)).to(DEVICE)\n",
        "            out = model.decode(ys, memory, tgt_mask)\n",
        "            out = out.transpose(0, 1)\n",
        "            prob = model.generator(out[:, -1])\n",
        "            prob = F.softmax(prob, dim=1)\n",
        "            top_k_prob, top_k_index = torch.topk(prob, k=k, dim=1)\n",
        "\n",
        "            for j in range(k):\n",
        "                next_word = top_k_index[0][j].item()\n",
        "                new_hypothesis.append({\n",
        "                    \"sequence\": torch.cat([ys,\n",
        "                                           torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0),\n",
        "                    \"score\": h[\"score\"] * top_k_prob[0][j].item(),\n",
        "                    \"memory\": memory\n",
        "                })\n",
        "        new_hypothesis = sorted(new_hypothesis, key=lambda x: x[\"score\"], reverse=True)\n",
        "        hypothesis = new_hypothesis[:k]\n",
        "        if hypothesis[0][\"sequence\"][-1] == EOS_IDX:\n",
        "            break\n",
        "    return hypothesis[0][\"sequence\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Traduction"
      ],
      "metadata": {
        "id": "CD5k0Tex87Fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, decoding_function: Callable, src_sentence: str, **kwargs):\n",
        "    \"\"\"\n",
        "    Translate sentence using any decoding strategy,\n",
        "\n",
        "    Args:\n",
        "        model: model,\n",
        "        decoding_function: callable function to generate sentences\n",
        "        src_sentence: source sentence\n",
        "        **kwargs: arguments of the decoding function\n",
        "\n",
        "    Returns:\n",
        "        translated sentence\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = decoding_function(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, **kwargs).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy())))\\\n",
        "              .replace(\"<bos>\", \"\")\\\n",
        "              .replace(\"<eos>\", \"\")"
      ],
      "metadata": {
        "id": "D6RppKzS3U7v"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGI_pjgroE-C"
      },
      "source": [
        "Maintenant comparons la traduction pour chacune des méthodes définies ci-dessus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "r2VtcCfWQ-Zu"
      },
      "outputs": [],
      "source": [
        "def multiple_translations(text):\n",
        "    \"\"\"\n",
        "    Generate multiple translations for a given sentence\n",
        "\n",
        "    Args:\n",
        "        text: sentence to translate\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Greedy :\", translate(transformer, greedy_decode, text))\n",
        "    print(\"Top K=10 :\", translate(transformer, top_k_sampling, text, k=10))\n",
        "    print(\"Top K=10 Temp=0.4 :\", translate(transformer, top_k_sampling_temperature, text, k=10, temperature=0.4))\n",
        "    print(\"Beam Search k=5 :\", translate(transformer, beam_search_sampling, text, k=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmbfdKMnnzIZ"
      },
      "source": [
        "### Exemple : \"*A group of people standing in front of an igloo .*\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSVOZ9w78L_Y",
        "outputId": "8efad73d-10ea-4977-d470-38289300b5c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy :  A group of people standing in front of an igloo . \n",
            "Top K=10 :  A group of people standing outside an igloo . \n",
            "Top K=10 Temp=0.4 :  A group of people standing in front of an igloo . \n",
            "Beam Search k=5 :  A group of people standing in front of an igloo . \n"
          ]
        }
      ],
      "source": [
        "multiple_translations(\"Eine Gruppe von Menschen steht vor einem Iglu .\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1olNQoIwn1aG"
      },
      "source": [
        "### Exemple : \"*This translation will never be as accurate as mine.*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yWIFX_F8MGA",
        "outputId": "f97c0038-127d-4b11-d4c5-96648e6b3edb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy :  This female athlete is being kissed by his talent . \n",
            "Top K=10 :  This female runner is skiing and gesturing . \n",
            "Top K=10 Temp=0.4 :  This female runner is skiing , while being washed . \n",
            "Beam Search k=5 :  This female athlete is skiing , wearing matching shoes . \n"
          ]
        }
      ],
      "source": [
        "multiple_translations(\"Diese Übersetzung wird niemals so genau sein wie meine .\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On observe que le modèle rencontre des difficultés à traduire des phrases hors données d'entrainements. Cela s'explique par le fait qu'il a été entrainé sur seulement 30k exemples ce qui est peu pour obtenir un vocabulaire correspondant à un language complet."
      ],
      "metadata": {
        "id": "EFPNIAT05kMm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrTVHKX0Lsxr"
      },
      "source": [
        "## (2 points) Compute the BLEU score of the model\n",
        "---\n",
        "\n",
        "Use the sacreBLEU implementation to evaluate your model and quantitatively compare the 4 implemented decoding approaches. Explain what all the output values mean (when using the `corpus_score` function)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9eadCojRgXn1"
      },
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import BLEU\n",
        "bleu = BLEU()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cL7ycERfu8BY"
      },
      "outputs": [],
      "source": [
        "test_data = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "# sources is the list of sentences of the source language\n",
        "# refs is the list of the same sentences in the target language\n",
        "sources, refs = zip(*test_data)\n",
        "refs = [refs]\n",
        "\n",
        "trans_greedy = [translate(transformer, greedy_decode, src) for src in sources]\n",
        "trans_topk = [translate(transformer, top_k_sampling, src, k=10) for src in sources]\n",
        "trans_topk_temp = [translate(transformer, top_k_sampling_temperature, src, k=10, temperature=0.4) for src in sources]\n",
        "trans_beam = [translate(transformer, beam_search_sampling, src, k=5) for src in sources]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OHMzQG1Eq3Z",
        "outputId": "edaa6221-8e62-4366-c2be-2e3fba762009"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy translation BLEU score :\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BLEU = 35.30 67.2/43.2/28.2/18.9 (BP = 1.000 ratio = 1.009 hyp_len = 13405 ref_len = 13289)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "print(\"Greedy translation BLEU score :\")\n",
        "bleu.corpus_score(trans_greedy, refs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqUPgeFKu8BY"
      },
      "source": [
        "- Le premier nombre correspond au score BLEU\n",
        "- les quatre nombres X/X/X/X representent la précision pour 1–4 ngram\n",
        "- BP (brevity penalty) représente le score qui pénalise les traductions trop petites\n",
        "- `ratio` indique le ratio entre la taille de l'hypothèse et la taille de la ref\n",
        "- `hyp_len` se réfère au nombre total de caractères pour le texte de l'hypothèse\n",
        "- `ref_len` est le nombre total de caractères du texte de référence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "z0VjPhUFFhvc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5b82ecc-3b8d-4d15-9125-3162ec003469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top K sampling translation BLEU score :\n",
            "BLEU = 28.97 61.9/36.6/22.4/13.9 (BP = 1.000 ratio = 1.015 hyp_len = 13489 ref_len = 13289)\n",
            "Top K sampling with temperature translation BLEU score :\n",
            "BLEU = 34.51 66.7/42.4/27.6/18.2 (BP = 1.000 ratio = 1.008 hyp_len = 13390 ref_len = 13289)\n",
            "Beam Search translation BLEU score :\n",
            "BLEU = 36.82 69.7/45.8/30.4/20.7 (BP = 0.978 ratio = 0.978 hyp_len = 12998 ref_len = 13289)\n"
          ]
        }
      ],
      "source": [
        "print(\"Top K sampling translation BLEU score :\")\n",
        "print(bleu.corpus_score(trans_topk, refs))\n",
        "\n",
        "print(\"Top K sampling with temperature translation BLEU score :\")\n",
        "print(bleu.corpus_score(trans_topk_temp, refs))\n",
        "\n",
        "print(\"Beam Search translation BLEU score :\")\n",
        "print(bleu.corpus_score(trans_beam, refs))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beam Search semble être sensiblement meilleur que les autres techniques de décodages."
      ],
      "metadata": {
        "id": "7f29KiZ59KpU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CbP8AlFv6b_v"
      },
      "execution_count": 20,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}